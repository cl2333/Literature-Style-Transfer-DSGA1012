{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_raw = open('original.txt').read()\n",
    "modern_raw = open('modern.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(data, sub_list=['ì','î','`', 'ó']):\n",
    "    for sub_char in sub_list:\n",
    "        data = re.sub(sub_char, '', data)\n",
    "        \n",
    "    data = re.sub('í', \"'\", data)\n",
    "    \n",
    "    # Delete empty lines\n",
    "    while ('\\n\\n' in data):\n",
    "        data = re.sub('\\n\\n', '\\n', data)\n",
    "    \n",
    "    # Remove blanks at end of paragraph\n",
    "    while (' \\n' in data):\n",
    "        data = re.sub(' \\n', '\\n', data)\n",
    "        \n",
    "    # Mask \\n to prevent from removing\n",
    "    data = re.sub('\\n', '<\\n>', data)\n",
    "    \n",
    "    # Seperate punctuations, insert space before and after \n",
    "    \n",
    "    data = re.sub('([.,!?;:])', r' \\1 ', data)\n",
    "    data = re.sub('\\s{2,}', ' ', data)\n",
    "    \n",
    "    # Put \\n back\n",
    "    data = re.sub('<\\n>', '\\n', data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_all_cleaned = process_text(original_raw)\n",
    "modern_all_cleaned = process_text(modern_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_all_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save processed files. (Make sure to delete original ones from directory first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('original_all_cleaned.txt', 'w').write(original_all_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('modern_all_cleaned.txt', 'w').write(modern_all_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split into train/valid/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Check original & modern text has same num of paragraph:', True)\n"
     ]
    }
   ],
   "source": [
    "check_equal = (len(original_all_cleaned.splitlines())==len(modern_all_cleaned.splitlines()))\n",
    "                   \n",
    "print('Check original & modern text has same num of paragraph:', check_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3269\n",
      "3269\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(original_all_cleaned.splitlines()) * 0.6)\n",
    "valid_size = int(len(original_all_cleaned.splitlines()) * 0.2) + 1\n",
    "test_size = int(len(original_all_cleaned.splitlines()) * 0.2) + 1\n",
    "\n",
    "print(len(original_all_cleaned.splitlines()))\n",
    "print(train_size + valid_size + test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ori_list = original_all_cleaned.splitlines()[: train_size]\n",
    "train_ori = '\\n'.join(train_ori_list)\n",
    "\n",
    "valid_ori_list = original_all_cleaned.splitlines()[train_size: train_size + valid_size]\n",
    "valid_ori = '\\n'.join(valid_ori_list)\n",
    "\n",
    "test_ori_list = original_all_cleaned.splitlines()[train_size + valid_size : ]\n",
    "test_ori = '\\n'.join(test_ori_list)\n",
    "\n",
    "train_mod_list = modern_all_cleaned.splitlines()[: train_size]\n",
    "train_mod = '\\n'.join(train_mod_list)\n",
    "\n",
    "valid_mod_list = modern_all_cleaned.splitlines()[train_size: train_size + valid_size]\n",
    "valid_mod = '\\n'.join(valid_mod_list)\n",
    "\n",
    "test_mod_list = modern_all_cleaned.splitlines()[train_size + valid_size : ]\n",
    "test_mod = '\\n'.join(test_mod_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('train.original.nltktok', 'w').write(train_ori)\n",
    "open('valid.original.nltktok', 'w').write(valid_ori)\n",
    "open('test.original.nltktok', 'w').write(test_ori)\n",
    "open('train.modern.nltktok', 'w').write(train_mod)\n",
    "open('valid.modern.nltktok', 'w').write(valid_mod)\n",
    "open('test.modern.nltktok', 'w').write(test_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. More cleaning with NLTK (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuweiwang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus = PlaintextCorpusReader(os.getcwd(), 'original_all_cleaned.txt')\n",
    "modern_corpus = PlaintextCorpusReader(os.getcwd(), 'modern_all_cleaned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total paragraphs in this corpus:', 3269)\n",
      "('Total sentences in this corpus: ', 7656)\n",
      "('First sentence in this corpus: ', [u'It', u'was', u'the', u'best', u'of', u'times', u',', u'it', u'was', u'the', u'worst', u'of', u'times', u',', u'it', u'was', u'the', u'age', u'of', u'wisdom', u',', u'it', u'was', u'the', u'age', u'of', u'foolishness', u',', u'it', u'was', u'the', u'epoch', u'of', u'belief', u',', u'it', u'was', u'the', u'epoch', u'ofincredulity', u',', u'it', u'was', u'the', u'season', u'of', u'Light', u',', u'it', u'was', u'the', u'season', u'of', u'Darkness', u',', u'it', u'wasthe', u'spring', u'of', u'hope', u',', u'it', u'was', u'the', u'winter', u'of', u'despair', u',', u'we', u'had', u'everything', u'before', u'us', u',', u'we', u'had', u'nothing', u'before', u'us', u',', u'we', u'were', u'all', u'going', u'direct', u'to', u'Heaven', u',', u'we', u'were', u'all', u'goingdirect', u'the', u'other', u'wayin', u'short', u',', u'the', u'period', u'was', u'so', u'far', u'like', u'the', u'present', u'period', u',', u'that', u'some', u'of', u'its', u'noisiest', u'authorities', u'insisted', u'on', u'its', u'being', u'received', u',', u'for', u'goodor', u'for', u'evil', u',', u'in', u'the', u'superlative', u'degree', u'of', u'comparison', u'only', u'.'])\n",
      "('Words in this corpus: ', [u'It', u'was', u'the', u'best', u'of', u'times', u',', ...])\n"
     ]
    }
   ],
   "source": [
    "original_para = original_corpus.paras()\n",
    "print(\"Total paragraphs in this corpus:\", len(original_para))\n",
    "\n",
    "original_sent = original_corpus.sents()\n",
    "print(\"Total sentences in this corpus: \", len(original_sent))\n",
    "print(\"First sentence in this corpus: \", original_sent[0])\n",
    "\n",
    "original_word = original_corpus.words()\n",
    "print(\"Words in this corpus: \", original_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check load data function (in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Shakespear text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data = open('sample',\"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_rows):\n",
    "\treturn [row.strip().lower().split(' ') for row in text_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'jumbled', 'confession', 'can', 'only', 'receive', 'a', 'jumbled', 'absolution', '.'], ['i', 'love', 'rich', \"capulet's\", 'daughter', '.'], [\"we're\", 'bound', 'to', 'each', 'other', 'in', 'every', 'possible', 'way', ',', 'except', 'we', 'need', 'you', 'to', 'marry', 'us', '.'], [\"i'll\", 'tell', 'you', 'more', 'later', 'about', 'when', 'and', 'where', 'we', 'met', ',', 'how', 'we', 'fell', 'in', 'love', ',', 'and', 'how', 'we', 'exchanged', 'promises', ',', 'but', 'now', \"i'm\", 'begging', 'you', ':', 'please', ',', 'agree', 'to', 'marry', 'us', 'today', '.'], ['holy', 'saint', 'francis', ',', 'this', 'is', 'a', 'drastic', 'change', '!']]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 New input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'was', 'the', 'best', 'of', 'times', ',', 'it', 'was', 'the', 'worst', 'of', 'times', ',', 'it', 'was', 'the', 'age', 'of', 'wisdom', ',', 'it', 'was', 'the', 'age', 'of', 'foolishness', ',', 'it', 'was', 'the', 'epoch', 'of', 'belief', ',', 'it', 'was', 'the', 'epoch', 'of', 'incredulity', ',', 'it', 'was', 'the', 'season', 'of', 'light', ',', 'it', 'was', 'the', 'season', 'of', 'darkness', ',', 'it', 'was', 'the', 'spring', 'of', 'hope', ',', 'it', 'was', 'the', 'winter', 'of', 'despair', ',', 'we', 'had', 'everything', 'before', 'us', ',', 'we', 'had', 'nothing', 'before', 'us', ',', 'we', 'were', 'all', 'going', 'direct', 'to', 'heaven', ',', 'we', 'were', 'all', 'going', 'direct', 'the', 'other', 'wayin', 'short', ',', 'the', 'period', 'was', 'so', 'far', 'like', 'the', 'present', 'period', ',', 'that', 'some', 'of', 'its', 'noisiest', 'authorities', 'insisted', 'on', 'its', 'being', 'received', ',', 'for', 'good', 'or', 'for', 'evil', ',', 'in', 'the', 'superlative', 'degree', 'of', 'comparison', 'only', '.'], ['there', 'were', 'a', 'king', 'with', 'a', 'large', 'jaw', 'and', 'a', 'queen', 'with', 'a', 'plain', 'face', ',', 'on', 'the', 'throne', 'of', 'england', ';', 'there', 'were', 'a', 'king', 'with', 'a', 'large', 'jaw', 'and', 'a', 'queen', 'with', 'a', 'fair', 'face', ',', 'on', 'the', 'throne', 'of', 'france', '.', 'in', 'both', 'countries', 'it', 'was', 'clearer', 'than', 'crystal', 'to', 'the', 'lords', 'of', 'the', 'state', 'preserves', 'of', 'loaves', 'and', 'fishes', ',', 'that', 'things', 'in', 'general', 'were', 'settled', 'for', 'ever', '.'], ['it', 'was', 'the', 'year', 'of', 'our', 'lord', 'one', 'thousand', 'seven', 'hundred', 'and', 'seventy-five', '.', 'spiritual', 'revelations', 'were', 'conceded', 'to', 'england', 'at', 'that', 'favoured', 'period', ',', 'as', 'at', 'this', '.', 'mrs', '.', 'southcott', 'had', 'recently', 'attained', 'her', 'five-and-twentieth', 'blessed', 'birthday', ',', 'of', 'whom', 'a', 'prophetic', 'private', 'in', 'the', 'life', 'guards', 'had', 'heralded', 'the', 'sublime', 'appearance', 'by', 'announcing', 'that', 'arrangements', 'were', 'made', 'for', 'the', 'swallowing', 'up', 'of', 'london', 'and', 'westminster', '.', 'even', 'the', 'cock-lane', 'ghost', 'had', 'been', 'laid', 'only', 'a', 'round', 'dozen', 'of', 'years', ',', 'after', 'rapping', 'out', 'its', 'messages', ',', 'as', 'the', 'spirits', 'of', 'this', 'very', 'year', 'last', 'past', '(supernaturally', 'deficient', 'in', 'originality)', 'rapped', 'out', 'theirs', '.', 'mere', 'messages', 'in', 'the', 'earthly', 'order', 'of', 'events', 'had', 'lately', 'come', 'to', 'the', 'english', 'crown', 'and', 'people', ',', 'from', 'a', 'congress', 'of', 'british', 'subjects', 'in', 'america', ':', 'which', ',', 'strange', 'to', 'relate', ',', 'have', 'proved', 'more', 'important', 'to', 'the', 'human', 'race', 'than', 'any', 'communications', 'yet', 'received', 'through', 'any', 'of', 'the', 'chickens', 'of', 'the', 'cock-lane', 'brood', '.']]\n"
     ]
    }
   ],
   "source": [
    "new_data = open('train.original.nltktok',\"r\").readlines()\n",
    "new_inputs = preprocess(new_data)\n",
    "print(new_inputs[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
